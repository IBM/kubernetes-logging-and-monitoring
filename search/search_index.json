{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Workshop Title \u00b6 Welcome to our workshop! In this workshop we'll be using foo to accomplish bar. The goals of this workshop are: Goals! Have fun! About this workshop \u00b6 The introductory page of the workshop is broken down into the following sections: Agenda Compatibility Technology Used Credits Agenda \u00b6 Lab 0: Pre-work Pre-work for the project Lab 1: Some Title Exercise to do the thing Compatibility \u00b6 This workshop has been tested on the following platforms: osName : version X, version Y Technology Used \u00b6 Brief Description Credits \u00b6 Full Name","title":"Welcome"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"#workshop-title","text":"Welcome to our workshop! In this workshop we'll be using foo to accomplish bar. The goals of this workshop are: Goals! Have fun!","title":"Workshop Title"},{"location":"#about-this-workshop","text":"The introductory page of the workshop is broken down into the following sections: Agenda Compatibility Technology Used Credits","title":"About this workshop"},{"location":"#agenda","text":"Lab 0: Pre-work Pre-work for the project Lab 1: Some Title Exercise to do the thing","title":"Agenda"},{"location":"#compatibility","text":"This workshop has been tested on the following platforms: osName : version X, version Y","title":"Compatibility"},{"location":"#technology-used","text":"Brief Description","title":"Technology Used"},{"location":"#credits","text":"Full Name","title":"Credits"},{"location":"logdna/lab-0/","text":"LogDNA Introduction \u00b6 Manage logs with LogDNA in IBM Cloud \u00b6 IBM Log Analysis with LogDNA allows you to capture your application and environment logs, filter out noisy or irrelevant log lines, alert, search, and archive your log data. You can build real-time dashboards with highly interactive graphs, including Counters, Gauges, Tables, and Time-Shifted Graphs. This exercise shows how the IBM Log Analysis with LogDNA service can be used to configure and access logs of a Kubernetes application that is deployed on IBM Cloud. You will deploy a Spring Java application to a cluster provisioned on IBM Cloud Kubernetes Service, configure a LogDNA agent, generate application logs and access worker logs, pod logs or network logs. Then, you will search, filter and visualize those logs through Log Analysis with LogDNA Web UI. The diagram below illustrates the service landscape used for this repo. Adopted from repo Spring PetClinic Microservice example running on Kubernetes(in Korean) , Analyze logs and monitor application health with LogDNA and Sysdig and IBM Cloud Patterns . Sample application architecture \u00b6 A simplified version of petclinic application is used in this repo. It includes four microservice components: api-gateway customers vets visits The complete version of petclinic application with microservice architecture is available here . Pre-requisites \u00b6 Claim your kubernetes cluster and complete the setup steps as described here . Exercise Steps \u00b6 Lab 1 - Deploy and Setup IBM Log Analysis with LogDNA Lab 2 - Deploy sample application to IKS cluster Lab 3 - Generate application log entries and analyze logs Lab 4 - Log streaming via LogDNA Related Links \u00b6 There is lots of great information, tutorials, articles, etc on the IBM Developer site as well as broader web. Here are a subset of good examples related to data understanding, visualization and processing: Log Analysis with LogDNA Documentation LogDNA Documentation LogDNA Blog","title":"Introduction"},{"location":"logdna/lab-0/#logdna-introduction","text":"","title":"LogDNA Introduction"},{"location":"logdna/lab-0/#manage-logs-with-logdna-in-ibm-cloud","text":"IBM Log Analysis with LogDNA allows you to capture your application and environment logs, filter out noisy or irrelevant log lines, alert, search, and archive your log data. You can build real-time dashboards with highly interactive graphs, including Counters, Gauges, Tables, and Time-Shifted Graphs. This exercise shows how the IBM Log Analysis with LogDNA service can be used to configure and access logs of a Kubernetes application that is deployed on IBM Cloud. You will deploy a Spring Java application to a cluster provisioned on IBM Cloud Kubernetes Service, configure a LogDNA agent, generate application logs and access worker logs, pod logs or network logs. Then, you will search, filter and visualize those logs through Log Analysis with LogDNA Web UI. The diagram below illustrates the service landscape used for this repo. Adopted from repo Spring PetClinic Microservice example running on Kubernetes(in Korean) , Analyze logs and monitor application health with LogDNA and Sysdig and IBM Cloud Patterns .","title":"Manage logs with LogDNA in IBM Cloud"},{"location":"logdna/lab-0/#sample-application-architecture","text":"A simplified version of petclinic application is used in this repo. It includes four microservice components: api-gateway customers vets visits The complete version of petclinic application with microservice architecture is available here .","title":"Sample application architecture"},{"location":"logdna/lab-0/#pre-requisites","text":"Claim your kubernetes cluster and complete the setup steps as described here .","title":"Pre-requisites"},{"location":"logdna/lab-0/#exercise-steps","text":"Lab 1 - Deploy and Setup IBM Log Analysis with LogDNA Lab 2 - Deploy sample application to IKS cluster Lab 3 - Generate application log entries and analyze logs Lab 4 - Log streaming via LogDNA","title":"Exercise Steps"},{"location":"logdna/lab-0/#related-links","text":"There is lots of great information, tutorials, articles, etc on the IBM Developer site as well as broader web. Here are a subset of good examples related to data understanding, visualization and processing: Log Analysis with LogDNA Documentation LogDNA Documentation LogDNA Blog","title":"Related Links"},{"location":"logdna/lab-1/","text":"Deploy and Setup IBM Log Analysis with LogDNA \u00b6 IBM Log Analysis with LogDNA allows you to capture your application and environment logs, filter out noisy or irrelevant log lines, alert, search, and archive your log data. You can build real-time dashboards with highly interactive graphs, including Counters, Gauges, Tables, and Time-Shifted Graphs. Step 1: Deploy IBM Log Analysis with LogDNA \u00b6 To deploy an instance of IBM Log Analysis with LogDNA , Login to IBM Cloud in a browser. Click here open the IBM Cloud Logging and view the IBM Log Analysis with LogDNA instances provisioned under your account. Select Create instance at the top-right corner. Select a location, for example us-east . (Pick the location where the IKS cluster resides.) Select a pricing plan . Lite plan is available. Enter a service name or accept the default. Click the Create button to create the new service instance. Step 2: Connect your Kubernetes cluster to LogDNA \u00b6 To collect logs from IKS cluster and related applications, LogDNA agent will be installed on each node of the cluster. Login to IBM Cloud in a browser. Locate and open the IKS cluster that you intend to collect logs from. Make sure the Overview tab is selected in the left pane. Click on the Connect button next to the Logging . This step connects your cluster to the LogDNA instance. Select the Region where LogDNA instance was provisioned, for example Dallas . Select your LogDNA instance. Uncheck the Use private endpoint box. Click the Connect button to install LogDNA agent on all IKS cluster nodes. The connect or the agent install step creates a set of resources under ibm-observe namespace. To view those changes, run the command as shown below from the CLI environment: kubectl get all -n ibm-observe kubectl get all -n ibm-observe NAME READY STATUS RESTARTS AGE pod/logdna-agent-2rqsd 1/1 Running 0 24s pod/logdna-agent-gz4mw 1/1 Running 0 24s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/logdna-agent 2 2 2 2 2 <none> 24s This shows two agnet pods in Running state, one per worker node. Step 3: Accessing LogDNA console \u00b6 Click on the Launch button to open the LogDNA console in a new browser window. The Dashboard should open up with some existing log statements from the cluster that pertains to the Kubernetes system pods. Check the options available under Filter dropdowns. LogDNA is ready to collect logs from applications in the cluster and the cluster itself. We will dive deeper into log analysis and the use of filters in the later labs.","title":"Lab 1. LogDNA Install"},{"location":"logdna/lab-1/#deploy-and-setup-ibm-log-analysis-with-logdna","text":"IBM Log Analysis with LogDNA allows you to capture your application and environment logs, filter out noisy or irrelevant log lines, alert, search, and archive your log data. You can build real-time dashboards with highly interactive graphs, including Counters, Gauges, Tables, and Time-Shifted Graphs.","title":"Deploy and Setup IBM Log Analysis with LogDNA"},{"location":"logdna/lab-1/#step-1-deploy-ibm-log-analysis-with-logdna","text":"To deploy an instance of IBM Log Analysis with LogDNA , Login to IBM Cloud in a browser. Click here open the IBM Cloud Logging and view the IBM Log Analysis with LogDNA instances provisioned under your account. Select Create instance at the top-right corner. Select a location, for example us-east . (Pick the location where the IKS cluster resides.) Select a pricing plan . Lite plan is available. Enter a service name or accept the default. Click the Create button to create the new service instance.","title":"Step 1: Deploy IBM Log Analysis with LogDNA"},{"location":"logdna/lab-1/#step-2-connect-your-kubernetes-cluster-to-logdna","text":"To collect logs from IKS cluster and related applications, LogDNA agent will be installed on each node of the cluster. Login to IBM Cloud in a browser. Locate and open the IKS cluster that you intend to collect logs from. Make sure the Overview tab is selected in the left pane. Click on the Connect button next to the Logging . This step connects your cluster to the LogDNA instance. Select the Region where LogDNA instance was provisioned, for example Dallas . Select your LogDNA instance. Uncheck the Use private endpoint box. Click the Connect button to install LogDNA agent on all IKS cluster nodes. The connect or the agent install step creates a set of resources under ibm-observe namespace. To view those changes, run the command as shown below from the CLI environment: kubectl get all -n ibm-observe kubectl get all -n ibm-observe NAME READY STATUS RESTARTS AGE pod/logdna-agent-2rqsd 1/1 Running 0 24s pod/logdna-agent-gz4mw 1/1 Running 0 24s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/logdna-agent 2 2 2 2 2 <none> 24s This shows two agnet pods in Running state, one per worker node.","title":"Step 2: Connect your Kubernetes cluster to LogDNA"},{"location":"logdna/lab-1/#step-3-accessing-logdna-console","text":"Click on the Launch button to open the LogDNA console in a new browser window. The Dashboard should open up with some existing log statements from the cluster that pertains to the Kubernetes system pods. Check the options available under Filter dropdowns. LogDNA is ready to collect logs from applications in the cluster and the cluster itself. We will dive deeper into log analysis and the use of filters in the later labs.","title":"Step 3: Accessing LogDNA console"},{"location":"logdna/lab-2/","text":"Deploy sample application to IKS cluster \u00b6 In this lab, you will deploy the Petclinic application into your kubernetes cluster. Check the lab introduction to learn more about this application. Logs from the application will be used for analysis in the next set of labs. Step 1: Deploy the Petclinic application \u00b6 Use the IBM Cloud Shell to deploy the sample Petclinic application. Get the source code from the Git repo Clone the repo. Run the commands listed from the the IBM Cloud Shell . git clone https://github.com/IBM/kubernetes-logging-and-monitoring.git cd kubernetes-logging-and-monitoring/src Deploy four microservices of the sample petclinic application by running the command below: kubectl create -f k8s/app/ $ kubectl create -f k8s/app/ deployment.apps/api-gateway created service/api-gateway created deployment.apps/customers created service/customers-service created deployment.apps/vets created service/vets-service created deployment.apps/visits created service/visits-service created This deploys Deployment and Service resources for each microservice component. Verify the deployment resources. Note that 4 pods and 4 services in addition to deployments and replicaset are created for this deployment. kubectl get all $ kubectl get all NAME READY STATUS RESTARTS AGE pod/api-gateway-575f59b7d8-7jhmc 1/1 Running 0 52s pod/customers-687749cfb-g8mkv 1/1 Running 0 52s pod/vets-6bb6655b7f-wkbcw 1/1 Running 0 52s pod/visits-784749c647-lczr6 1/1 Running 0 52s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/api-gateway NodePort 172.21.20.70 <none> 80:32002/TCP 52s service/customers-service NodePort 172.21.99.121 <none> 80:32003/TCP 52s service/kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 12h service/vets-service NodePort 172.21.63.239 <none> 80:32005/TCP 52s service/visits-service NodePort 172.21.156.238 <none> 80:32004/TCP 52s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/api-gateway 1/1 1 1 52s deployment.apps/customers 1/1 1 1 52s deployment.apps/vets 1/1 1 1 52s deployment.apps/visits 1/1 1 1 52s NAME DESIRED CURRENT READY AGE replicaset.apps/api-gateway-575f59b7d8 1 1 1 52s replicaset.apps/customers-687749cfb 1 1 1 52s replicaset.apps/vets-6bb6655b7f 1 1 1 52s replicaset.apps/visits-784749c647 1 1 1 52s Step 2: Deploy Ingress resource \u00b6 When deployed the sample application to a non-Lite tier IKS cluster, it's possible to expose the application with an external hostname. Retrieve Ingress Subdomain . Ensure the value for variable MYCLUSTER is set. echo $MYCLUSTER Lookup the value for ingress subdomain and set that value to the environment variable INGRESS_SUBDOMAIN . ibmcloud ks cluster get -c $MYCLUSTER export INGRESS_SUBDOMAIN = ` ibmcloud ks cluster get -c $MYCLUSTER --output json | jq -r '.ingressHostname' ` echo \"\\n Ingress subdomain: ${ INGRESS_SUBDOMAIN } \" Retrieving cluster c0i8ggbd0hnsl5uiaelg... OK Name: rj-iks-monitoring ID: c0i8ggbd0hnsl5uiaelg State: normal Status: All Workers Normal Created: 2021-02-11T01:28:33+0000 Location: dal10 Pod Subnet: 172.30.0.0/16 Service Subnet: 172.21.0.0/16 Master URL: https://c108.us-south.containers.cloud.ibm.com:30952 Public Service Endpoint URL: https://c108.us-south.containers.cloud.ibm.com:30952 Private Service Endpoint URL: https://c108.private.us-south.containers.cloud.ibm.com:30952 Master Location: Dallas Master Status: Ready (13 hours ago) Master State: deployed Master Health: normal Ingress Subdomain: rj-iks-monitoring-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud Ingress Secret: rj-iks-monitoring-2bef1f4b4097001da9502000c44fc2b2-0000 Ingress Status: healthy Ingress Message: All Ingress components are healthy Workers: 2 Worker Zones: dal10 Version: 1.18.15_1538 Creator: - Monitoring Dashboard: - Resource Group ID: ee06e1f30dde43d4a01d71be7501d9bb Resource Group Name: rj-resources Ingress subdomain: rj-iks-monitoring-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud Set the application domain name Replace <INGRESS_SUBDOMAIN> in k8s/ingress.yaml file with the value that you retrieved in the previous step by running the following command: sed -i \"s/<INGRESS_SUBDOMAIN>/ ${ INGRESS_SUBDOMAIN } /\" k8s/ingress.yaml Users on Mac: sed -i \"\" \"s/<INGRESS_SUBDOMAIN>/ ${ INGRESS_SUBDOMAIN } /\" k8s/ingress.yaml Verify the igress value was substituted correctly: $ more k8s/ingress.yaml apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 name: api-ingress spec: rules: - host: petclinic.rj-iks-monitoring-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud http: paths: ... Deploy the Ingress resource kubectl create -f k8s/ingress.yaml $ kubectl create -f k8s/ingress.yaml ingress.networking.k8s.io/api-ingress created Step 3: Verify petclinic application \u00b6 If everything goes as planned, the petclinic application can be accessed at https://petclinic.<INGRESS_SUBDOMAIN> . By default, an internal database is used to stored data. Get the application URL by running the comand below. echo \"https://petclinic. ${ INGRESS_SUBDOMAIN } \" Copy the URL into a new browser window to view the Petclinc application landing page. Step 4: Deploy MYSQL database to the IKS cluster (Optionally) \u00b6 Instead of running the petclinic application on an internal database, you may choose to deploy an instance of MYSQL database on the same IKS cluster. Step 4.1 - Prepare Persisent Volume \u00b6 There are various persistent storage options to store the data of MYSQL DATABASE, local storage and cloud storage and etc. For the simplicity, the local file system system on the Node server is used for this repo. Execute the command below to create a 5Gi local-volume. kubectl create -f k8s/mysql/local-volumes.yaml Step 4.2 - Create a secret storing MYSQL credential \u00b6 User and password of MYSQL database is stored in a secret resource for security reason. kubectl create -f k8s/mysql/mysql-secret.yaml Step 4.3 - Deploy MYSQL database \u00b6 One deployment, one service and one persistent-volume-claim resources are created when deploying MYSQL database. kubectl create -f k8s/mysql/mysql.yaml Step 4.4 - Populate MYSQL database \u00b6 To populate MYSQL database running on the cluster, Retrieve the pod information where MYSQL database is running. kubectl get pod -l app = mysql NAME READY STATUS RESTARTS AGE mysql-6d87765586-2q7sn 1 /1 Running 0 19h Store the pod name of MYSQL. export MYSQL_POD = <MYSQL POD NAME> Copy SQL files to the pod. kubectl cp k8s/mysql/sql/mysql-schema.sql $MYSQL_POD :/tmp/ kubectl cp k8s/mysql/sql/mysql-data.sql $MYSQL_POD :/tmp/ Populate MYSQL database kubectl exec $MYSQL_POD -- sh -c 'mysql -uroot -ppetclinic petclinic < /tmp/mysql-schema.sql' kubectl exec $MYSQL_POD -- sh -c 'mysql -uroot -ppetclinic petclinic < /tmp/mysql-data.sql' Retrieve data from MYSQL database for verification. kubectl exec $MYSQL_POD -- sh -c 'mysql -u root -ppetclinic -e \"select * from vets\" petclinic' mysql: [Warning] Using a password on the command line interface can be insecure. id first_name last_name 1 James Carter 2 Helen Leary 3 Linda Douglas 4 Rafael Ortega 5 Henry Stevens 6 Sharon Jenkins Step 5: Run sample application on MYSQL database \u00b6 MYSQL database has been successfully deployed in the same IKS cluster. Now, you are goint to run the sample petclinic application on MYSQL database instead of the internal database. Step 5.1 - Store database connection information in configMap \u00b6 To store MYSQL database connection information in configMap resource, kubectl create -f k8s/mysql/mysql-configmap.yaml Step 5.2 - Modify sample application deployment to run on MySQL DB \u00b6 kubectl apply -f k8s/mysql/mysql-customers-service.yaml kubectl apply -f k8s/mysql/mysql-vets-service.yaml kubectl apply -f k8s/mysql/mysql-visits-service.yaml Step 6: Verify petclinic application \u00b6 Retrieve Ingress Subdomain . echo https://petclinic. $INGRESS_SUBDOMAIN $ echo https://petclinic.$INGRESS_SUBDOMAIN https://petclinic.leez-iks03-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud Access the petclinic application via https://petclinic.$INGRESS_SUBDOMAIN . Now, it's running on MYSQL database instead of the internal database.","title":"Lab 2. Deploy Application"},{"location":"logdna/lab-2/#deploy-sample-application-to-iks-cluster","text":"In this lab, you will deploy the Petclinic application into your kubernetes cluster. Check the lab introduction to learn more about this application. Logs from the application will be used for analysis in the next set of labs.","title":"Deploy sample application to IKS cluster"},{"location":"logdna/lab-2/#step-1-deploy-the-petclinic-application","text":"Use the IBM Cloud Shell to deploy the sample Petclinic application. Get the source code from the Git repo Clone the repo. Run the commands listed from the the IBM Cloud Shell . git clone https://github.com/IBM/kubernetes-logging-and-monitoring.git cd kubernetes-logging-and-monitoring/src Deploy four microservices of the sample petclinic application by running the command below: kubectl create -f k8s/app/ $ kubectl create -f k8s/app/ deployment.apps/api-gateway created service/api-gateway created deployment.apps/customers created service/customers-service created deployment.apps/vets created service/vets-service created deployment.apps/visits created service/visits-service created This deploys Deployment and Service resources for each microservice component. Verify the deployment resources. Note that 4 pods and 4 services in addition to deployments and replicaset are created for this deployment. kubectl get all $ kubectl get all NAME READY STATUS RESTARTS AGE pod/api-gateway-575f59b7d8-7jhmc 1/1 Running 0 52s pod/customers-687749cfb-g8mkv 1/1 Running 0 52s pod/vets-6bb6655b7f-wkbcw 1/1 Running 0 52s pod/visits-784749c647-lczr6 1/1 Running 0 52s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/api-gateway NodePort 172.21.20.70 <none> 80:32002/TCP 52s service/customers-service NodePort 172.21.99.121 <none> 80:32003/TCP 52s service/kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 12h service/vets-service NodePort 172.21.63.239 <none> 80:32005/TCP 52s service/visits-service NodePort 172.21.156.238 <none> 80:32004/TCP 52s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/api-gateway 1/1 1 1 52s deployment.apps/customers 1/1 1 1 52s deployment.apps/vets 1/1 1 1 52s deployment.apps/visits 1/1 1 1 52s NAME DESIRED CURRENT READY AGE replicaset.apps/api-gateway-575f59b7d8 1 1 1 52s replicaset.apps/customers-687749cfb 1 1 1 52s replicaset.apps/vets-6bb6655b7f 1 1 1 52s replicaset.apps/visits-784749c647 1 1 1 52s","title":"Step 1: Deploy the Petclinic application"},{"location":"logdna/lab-2/#step-2-deploy-ingress-resource","text":"When deployed the sample application to a non-Lite tier IKS cluster, it's possible to expose the application with an external hostname. Retrieve Ingress Subdomain . Ensure the value for variable MYCLUSTER is set. echo $MYCLUSTER Lookup the value for ingress subdomain and set that value to the environment variable INGRESS_SUBDOMAIN . ibmcloud ks cluster get -c $MYCLUSTER export INGRESS_SUBDOMAIN = ` ibmcloud ks cluster get -c $MYCLUSTER --output json | jq -r '.ingressHostname' ` echo \"\\n Ingress subdomain: ${ INGRESS_SUBDOMAIN } \" Retrieving cluster c0i8ggbd0hnsl5uiaelg... OK Name: rj-iks-monitoring ID: c0i8ggbd0hnsl5uiaelg State: normal Status: All Workers Normal Created: 2021-02-11T01:28:33+0000 Location: dal10 Pod Subnet: 172.30.0.0/16 Service Subnet: 172.21.0.0/16 Master URL: https://c108.us-south.containers.cloud.ibm.com:30952 Public Service Endpoint URL: https://c108.us-south.containers.cloud.ibm.com:30952 Private Service Endpoint URL: https://c108.private.us-south.containers.cloud.ibm.com:30952 Master Location: Dallas Master Status: Ready (13 hours ago) Master State: deployed Master Health: normal Ingress Subdomain: rj-iks-monitoring-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud Ingress Secret: rj-iks-monitoring-2bef1f4b4097001da9502000c44fc2b2-0000 Ingress Status: healthy Ingress Message: All Ingress components are healthy Workers: 2 Worker Zones: dal10 Version: 1.18.15_1538 Creator: - Monitoring Dashboard: - Resource Group ID: ee06e1f30dde43d4a01d71be7501d9bb Resource Group Name: rj-resources Ingress subdomain: rj-iks-monitoring-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud Set the application domain name Replace <INGRESS_SUBDOMAIN> in k8s/ingress.yaml file with the value that you retrieved in the previous step by running the following command: sed -i \"s/<INGRESS_SUBDOMAIN>/ ${ INGRESS_SUBDOMAIN } /\" k8s/ingress.yaml Users on Mac: sed -i \"\" \"s/<INGRESS_SUBDOMAIN>/ ${ INGRESS_SUBDOMAIN } /\" k8s/ingress.yaml Verify the igress value was substituted correctly: $ more k8s/ingress.yaml apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 name: api-ingress spec: rules: - host: petclinic.rj-iks-monitoring-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud http: paths: ... Deploy the Ingress resource kubectl create -f k8s/ingress.yaml $ kubectl create -f k8s/ingress.yaml ingress.networking.k8s.io/api-ingress created","title":"Step 2: Deploy Ingress resource"},{"location":"logdna/lab-2/#step-3-verify-petclinic-application","text":"If everything goes as planned, the petclinic application can be accessed at https://petclinic.<INGRESS_SUBDOMAIN> . By default, an internal database is used to stored data. Get the application URL by running the comand below. echo \"https://petclinic. ${ INGRESS_SUBDOMAIN } \" Copy the URL into a new browser window to view the Petclinc application landing page.","title":"Step 3: Verify petclinic application"},{"location":"logdna/lab-2/#step-4-deploy-mysql-database-to-the-iks-cluster-optionally","text":"Instead of running the petclinic application on an internal database, you may choose to deploy an instance of MYSQL database on the same IKS cluster.","title":"Step 4: Deploy MYSQL database to the IKS cluster (Optionally)"},{"location":"logdna/lab-2/#step-41-prepare-persisent-volume","text":"There are various persistent storage options to store the data of MYSQL DATABASE, local storage and cloud storage and etc. For the simplicity, the local file system system on the Node server is used for this repo. Execute the command below to create a 5Gi local-volume. kubectl create -f k8s/mysql/local-volumes.yaml","title":"Step 4.1 - Prepare Persisent Volume"},{"location":"logdna/lab-2/#step-42-create-a-secret-storing-mysql-credential","text":"User and password of MYSQL database is stored in a secret resource for security reason. kubectl create -f k8s/mysql/mysql-secret.yaml","title":"Step 4.2 - Create a secret storing MYSQL credential"},{"location":"logdna/lab-2/#step-43-deploy-mysql-database","text":"One deployment, one service and one persistent-volume-claim resources are created when deploying MYSQL database. kubectl create -f k8s/mysql/mysql.yaml","title":"Step 4.3 - Deploy MYSQL database"},{"location":"logdna/lab-2/#step-44-populate-mysql-database","text":"To populate MYSQL database running on the cluster, Retrieve the pod information where MYSQL database is running. kubectl get pod -l app = mysql NAME READY STATUS RESTARTS AGE mysql-6d87765586-2q7sn 1 /1 Running 0 19h Store the pod name of MYSQL. export MYSQL_POD = <MYSQL POD NAME> Copy SQL files to the pod. kubectl cp k8s/mysql/sql/mysql-schema.sql $MYSQL_POD :/tmp/ kubectl cp k8s/mysql/sql/mysql-data.sql $MYSQL_POD :/tmp/ Populate MYSQL database kubectl exec $MYSQL_POD -- sh -c 'mysql -uroot -ppetclinic petclinic < /tmp/mysql-schema.sql' kubectl exec $MYSQL_POD -- sh -c 'mysql -uroot -ppetclinic petclinic < /tmp/mysql-data.sql' Retrieve data from MYSQL database for verification. kubectl exec $MYSQL_POD -- sh -c 'mysql -u root -ppetclinic -e \"select * from vets\" petclinic' mysql: [Warning] Using a password on the command line interface can be insecure. id first_name last_name 1 James Carter 2 Helen Leary 3 Linda Douglas 4 Rafael Ortega 5 Henry Stevens 6 Sharon Jenkins","title":"Step 4.4 - Populate MYSQL database"},{"location":"logdna/lab-2/#step-5-run-sample-application-on-mysql-database","text":"MYSQL database has been successfully deployed in the same IKS cluster. Now, you are goint to run the sample petclinic application on MYSQL database instead of the internal database.","title":"Step 5: Run sample application on MYSQL database"},{"location":"logdna/lab-2/#step-51-store-database-connection-information-in-configmap","text":"To store MYSQL database connection information in configMap resource, kubectl create -f k8s/mysql/mysql-configmap.yaml","title":"Step 5.1 - Store database connection information in configMap"},{"location":"logdna/lab-2/#step-52-modify-sample-application-deployment-to-run-on-mysql-db","text":"kubectl apply -f k8s/mysql/mysql-customers-service.yaml kubectl apply -f k8s/mysql/mysql-vets-service.yaml kubectl apply -f k8s/mysql/mysql-visits-service.yaml","title":"Step 5.2 - Modify sample application deployment to run on MySQL DB"},{"location":"logdna/lab-2/#step-6-verify-petclinic-application","text":"Retrieve Ingress Subdomain . echo https://petclinic. $INGRESS_SUBDOMAIN $ echo https://petclinic.$INGRESS_SUBDOMAIN https://petclinic.leez-iks03-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud Access the petclinic application via https://petclinic.$INGRESS_SUBDOMAIN . Now, it's running on MYSQL database instead of the internal database.","title":"Step 6: Verify petclinic application"},{"location":"logdna/lab-3/","text":"Analysing logs using LogDNA \u00b6 Generate application log entries \u00b6 The Petclinic application was de deployed in the previous exercice. Let's articially create some traffic in order to generate log statements. For simplicity, a script will be executed in api-gateway pod to send REST requests to all 4 service components of the petclinic application. Log entries will be generated for each service component. Step 1 - Send requests to the sample application \u00b6 Prior to sending the requests to the application, we will take a look at the pod logs and start watching the logs. This should provide an understanding of what comes in as log statements. To generate application log entries, Go back to CLI terminal and get a listing of pods. Then start watching the logs in separate CLI terminal sessions. kubectl get pods $ kubectl get pods NAME READY STATUS RESTARTS AGE api-gateway-575f59b7d8-bjhp8 1/1 Running 0 3h2m customers-687749cfb-d2zhq 1/1 Running 0 3h2m vets-6bb6655b7f-v6ppc 1/1 Running 0 3h2m visits-784749c647-tdpvv 1/1 Running 0 3h2m Watch logs for each of those pods using the follwing command: kubectl logs -f <pod-name> For instance the result of wathcing the api-gateway pod should like this: $ kubectl logs -f api-gateway-575f59b7d8-bjhp8 6.4.jar!/, jar:file:/app.jar!/BOOT-INF/lib/jquery-3.1.1-1.jar!/, jar:file:/app.jar!/BOOT-INF/lib/bootstrap-3.3.7-1.jar!/, jar:file:/app.jar!/BOOT-INF/lib/angular-ui-router-1.0.3 .jar!/, jar:file:/app.jar!/BOOT-INF/lib/webjars-locator-core-0.35.jar!/, jar:file:/app.jar!/BOOT-INF/lib/slf4j-api-1.7.26.jar!/, jar:file:/app.jar!/BOOT-INF/lib/commons-lang3-3. 8.1.jar!/, jar:file:/app.jar!/BOOT-INF/lib/commons-compress-1.9.jar!/, jar:file:/app.jar!/BOOT-INF/lib/jackson-core-2.9.8.jar!/] . ____ _ __ _ _ /\\\\ / ___'_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.1.4.RELEASE) 2021-02-11 19:58:03.730 INFO 1 --- [ main] o.s.s.p.api.ApiGatewayApplication : Starting ApiGatewayApplication v2.1.4 on api-gateway-575f59b7d8-bjhp8 with PID 1 (/app.jar started by root in /) 2021-02-11 19:58:03.731 DEBUG 1 --- [ main] o.s.s.p.api.ApiGatewayApplication : Running with Spring Boot v2.1.4.RELEASE, Spring v5.1.6.RELEASE 2021-02-11 19:58:03.731 INFO 1 --- [ main] o.s.s.p.api.ApiGatewayApplication : The following profiles are active: common,docker ..... ..... 2021-02-11 19:59:24.943 DEBUG 1 --- [nio-8080-exec-3] o.s.w.s.handler.SimpleUrlHandlerMapping : Mapped to ResourceHttpRequestHandler [\"classpath:/META-INF/resources/\", \"classpa th:/resources/\", \"classpath:/static/\", \"classpath:/public/\", \"/\"] 2021-02-11 19:59:24.943 DEBUG 1 --- [nio-8080-exec-1] o.s.w.s.handler.SimpleUrlHandlerMapping : Mapped to ResourceHttpRequestHandler [\"classpath:/META-INF/resources/\", \"classpa th:/resources/\", \"classpath:/static/\", \"classpath:/public/\", \"/\"] 2021-02-11 19:59:24.949 DEBUG 1 --- [nio-8080-exec-2] o.s.w.s.handler.SimpleUrlHandlerMapping : Mapped to ResourceHttpRequestHandler [\"classpath:/META-INF/resources/\", \"classpa th:/resources/\", \"classpath:/static/\", \"classpath:/public/\", \"/\"] 2021-02-11 19:59:24.967 DEBUG 1 --- [nio-8080-exec-3] o.s.web.servlet.DispatcherServlet : Completed 200 OK Go back to one of CLI terminal. Set the HOST to the Petclinic application URL. export HOST = \"https://petclinic. $INGRESS_SUBDOMAIN \" echo $HOST Run the follwing script to generate the log statements. This script will invoke the landing page, vets, owners and pet services in a loop. for i in ` seq 1 100 ` ; \\ do \\ echo -e \"\\n ======= Loop count: $i ========= \\nCalling owners:\" && \\ wget -q -O - \" ${ HOST } /api/customer/owners\" && \\ echo -e \"\\n Calling vets:\" && \\ wget -q -O - \" ${ HOST } /api/vet/vets\" && \\ echo -e \"\\n Calling visits:\" && \\ wget -q -O - \" ${ HOST } /api/visit/pets/visits?petId=7\" && \\ echo -e \"\\n Calling welcome:\" && \\ wget -q -O - \" ${ HOST } /#\\!/welcome\" ; \\ done ======= Loop count: 2 ========= Calling owners: [{\"id\":1,\"firstName\":\"George\",\"lastName\":\"Franklin\",\"address\":\"110 W. Liberty St.\",\"city\":\"Madison\",\"telephone\":\"6085551023\",\"pets\":[{\"id\":1,\"name\":\"Leo\",\"birthDate\":\"2010-09-07\",\"type\":{\"id\":1,\"name\":\"cat\"}}]},{\"id\":2,\"firstName\":\"Betty\",\"lastName\":\"Davis\",\"address\":\"638 Cardinal Ave.\",\"city\":\"Sun Prairie\",\"telephone\":\"6085551749\",\"pets\":[{\"id\":2,\"name\":\"Basil\",\"birthDate\":\"2012-08-06\",\"type\":{\"id\":6,\"name\":\"hamster\"}}]},{\"id\":3,\"firstName\":\"Eduardo\",\"lastName\":\"Rodriquez\",\"address\":\"2693 Commerce St.\",\"city\":\"McFarland\",\"telephone\":\"6085558763\",\"pets\":[{\"id\":4,\"name\":\"Jewel\",\"birthDate\":\"2010-03-07\",\"type\":{\"id\":2,\"name\":\"dog\"}},{\"id\":3,\"name\":\"Rosy\",\"birthDate\":\"2011-04-17\",\"type\":{\"id\":2,\"name\":\"dog\"}}]},{\"id\":4,\"firstName\":\"Harold\",\"lastName\":\"Davis\",\"address\":\"563 Friendly St.\",\"city\":\"Windsor\",\"telephone\":\"6085553198\",\"pets\":[{\"id\":5,\"name\":\"Iggy\",\"birthDate\":\"2010-11-30\",\"type\":{\"id\":3,\"name\":\"lizard\"}}]},{\"id\":5,\"firstName\":\"Peter\",\"lastName\":\"McTavish\",\"address\":\"2387 S. ......... ......... <body class=\"container\"> <layout-nav></layout-nav> <div class=\"container-fluid\"> <div class=\"container xd-container\"> <div ui-view=\"\"></div> </div> </div> <layout-footer></layout-footer> </body> </html> Switch to the CLI sessions setup for watching the los to view the logging activity. Work with your logs via LogDNA \u00b6 We are not ready to effectively analyze the application logs via LogDNA. Step 1: Open LogDNA console \u00b6 Open the LogDNA console like we did in the prevous lab. LogDNA console displays everything by default. LogDNA live-streams all log entries by default. You may turn OFF/ON LIVE log stream via the LIVE toggle button at the bottom right corner. Step 2: Search for specific terms in logs \u00b6 If you know what you are looking for, it's easy and convenient to search log entries in LogDNA. In the Search input box located at the bottom of the page in the LogDNA UI, enter GET \"/owners\" and press ENTER. Review the search result. All remaining log entries contains string GET \"/owners\" . Select any log entry and expand it by click the arraw in the front of the line. This displays detail information of the log entry. Select View in context . The log line will be displayed in context of other log lines from that host, app, or both. This information is helpful when troubleshooting a problem. Select EVERYTHING in the top-left corner to clear the searching criteria and restore all log entries. In the Search input box located at the bottom of the page in the LogDNA UI, enter level:error and press ENTER. Review the search result and only error log entries remain. Select EVERYTHING in the top-left corner to clear the searching criteria and restore all log entries. Enter 2 mins ago in the Jump to timeframe input box and press ENTER. LogDNA jumps to a specific timeframe, Click the icon next to the input box to find the other time formats within your retention period. Select the Toggle Viewer Tools icon at the bottom-right. Enter error as your highlight term in the first field and press ENTER. Enter container as your highlight term in the second field and press ENTER. Click the Toggle Viewer Tools icon to close the pop-up window. Click on Toggle Timeline icon to see log entries at a specific time of a day. Step 3 - Filter logs for a specific container \u00b6 You can filter logs by tags, sources, containers or levels. Tags: related to IKS clusters Sources: related to IKS pods Containers: related to IKS containers. Since one container typically runs one microservice per cloud native development best practice, each container should represent one microservice. For example, customers container represents customers service component. Levels: related to log level, for example error or debug. To filter logs, On LogDNA UI, select EVERYTHING in the top left corner to clear the searching criteria and restore all log entries. Select Tags dropdown list on the top. You can filter log entries related to a single cluster or a set of clusters. For this exercise, you don't have to change anything. Select Sources dropdown list, you may filter log entries related to one Kubernetes pod or a set of pods. For this exercise, you don't have to change anything. Select Apps dropdown list, you may filter log entries related to one Kubernetes container or a set of containers. Select Customers checkbox under the Containers section and Apply . Now, you see log entries for Customers service component only. Most of the entries are for DEBUG purpose. Select Levels dropdown list, you may filter log entries related to one log level or a set of log levels. Select everything except DEBUG log level and Apply . You may see no log entry or a few lines of log entries, depending on your log entries. Step 4 - Create a new view \u00b6 You may create a new view to save your current filter. Click Unsaved View on the LogDNA UI and select Save as new view . In the Name field, enter My CUSTOMERS service component . Select Save View . Step 5 - Visualize logs with graphs and breakdowns \u00b6 In this section, you will create a board and then add a graph with a breakdown to visualize the app level data. A board is a collection of graphs and breakdowns. In the left pane, select the Boards icon and then select NEW BOARD . Click Edit icon ( Pencil icon next to the New Board title) on the top bar. In the Name field, enter Debug Board . Click Save . Click Add Graph . Under the Graph a field section, select level in the first field. Second field Field Value appears. Select debug in the Field Value field. Click Add Graph . Counts is selected as your metric by default. The graph shows the number of log entries in the interval over last 24 hours. Extend the graph by clicking on the downn-arrow below the graph. Select Histogram as your breakdown type . Choose app in the second field. Select Add Breakdown to see a breakdown with all the apps you logged. Step 6 - Alerts \u00b6 You can trigger alerts in LogDNA whenever log lines appear in a custom view. The sample alert configuration in this section sends an email for every 3 new log entries that meet searching criteria. Step 6.1 - Create custom view \u00b6 To create an alert in LogDNA, Navigate to the homepage of the LogDNA. Select EVERYTHING . Enter GET \"/owners\" in the Search field and press ENTER. Click Unsaved View button on the top-left and select Save as new view . Enter REST Call in the Name field. Select Save View button. Step 6.2 - Create alert \u00b6 Select Settings in the left pane. Select ALERTS tab. Select Add Preset button. Enter customer-api-called as the Preset name . Select Email option. Triggers the alert when there are 3 Lines appears in 30 seconds . Send an alert At the end of 30 seconds . Turn on Custom schedule and review the settings. Make change if necessary. Enter your email address. Select your Timezone . Click Test button next to the Email . You should receive an email from LogDNA Alerts with the following testing contents. Test View \ud83d\udd14 2 test lines matched Feb 08 17:45:56 logdna alert_tester This is where your lines will show up Feb 08 17:45:56 logdna alert_tester After matching at least 3 lines in a 30 second period, we'll send an alert to this email with all the matched lines Save Alert . Step 6.3 - Attach alert and custom view \u00b6 To link alert and custom view in LogDNA, Select Views tab in the left pane. Select REST Call custom view. Click the REST Call dropdown menu on the top-left. Select Attach an alert option. Select customer-api-called alert. Save Alert . Step 6.4 - Trigger alert \u00b6 To trigger the LogDNA alert, you'll generate additional application log entries. Go back to IBM Cloud Shell terminal. Copy/paste and Execute the following command in the pod. The script sends 3 requests to customer microservice. for i in `seq 1 3` ; \\ do \\ echo -e \"\\n ======= Loop count: $i ========= \\nCalling customer owners:\" && \\ wget -q -O - \"${HOST}/api/customer/owners\" && \\ done Verify you received an email from LogDNA Alerts . REST Call \ud83d\udd14 (end of duration) 3 lines matched within 30 seconds Feb 08 19:14:01 customers-98d7b966c-8wx5x customers [DEBUG] 1 --- [io-8080-exec-10] o.s.web.servlet.DispatcherServlet : GET \"/owners\", parameters={} Feb 08 19:14:01 customers-98d7b966c-8wx5x customers [DEBUG] 1 --- [nio-8080-exec-8] o.s.web.servlet.DispatcherServlet : GET \"/owners\", parameters={} Feb 08 19:14:01 customers-98d7b966c-8wx5x customers [DEBUG] 1 --- [nio-8080-exec-9] o.s.web.servlet.DispatcherServlet : GET \"/owners\", parameters={} Step 7 - Review the log format \u00b6 When you launch the IBM Log Analysis with LogDNA web UI, log entries are displayed in a predefined format. You can modify how the log entries are displayed. Note: Configuration changes of the log format in this section will affect all defined views. It's possible to change log format for individual view. Select the Settings icon in the left pane. Select USER PREFERENCES . Select Log Format tab. Review the default log format and available log components. Do not make any change for this exercise. Change the log viewer text size by using the slider. To add items to log view, drag the available items from the bottom line to the top line. To rearrange the order of the items, drag and drop the items in the top line until you have your desired view. Step 8 - Configure exclusion rules \u00b6 In an IBM Log Analysis with LogDNA instance, you can configure exclusion rules through the LogDNA web UI to stop logs from counting against your data usage quota and from being stored for search. Step 8.1 - Create exclusion rules \u00b6 Select the Settings icon in the left pane. Select Usage and then Exclusion Rules . Click Add Rule button. The Create Rule section opens. Enter myExclusionRule in the field What is this rule for? . In the Apps field, select customers . In the Query field, enter level:debug . The query exludes the Debug log entries. Select Preserve these lines for live-tail and alerting to show through the live tail the log lines that are excluded. Notice that you can still use these log lines to set up an alert. Click Save . Step 8.2 - Test exclusion rules \u00b6 To test the exclusion rule, Go back to the LogDNA homepage. Select EVERYTHING . Click Apps dropdown and filter the logs by selecting customers checkbox under Containers . Apply . Go to IBM Cloud Shell terminal. Copy/paste and Execute the following command in the pod. for i in `seq 1 3` ; \\ do \\ echo -e \"\\n ======= Loop count: $i ========= \\nCalling customer owners:\" && \\ wget -q -O - \"${HOST}/api/customer/owners\" && \\ done Go back to LogDNA UI, you should receive no new log entry by looking at the timestamp of the last log entry. This confirms that the newly generated log entries do not count against your data usage. Verify you received an email from LogDNA Alerts . This confirms that you can still receive alert based on the newly generated log entries although they don't count against your data usage. Step 8.3 - Delete exclusion rules \u00b6 The exclusion rule myExclusionRule should be deleted as it was created fopr demostration reason. Otherwise, your LogDNA may not as you expect. Select the Settings icon in the left pane. Select Usage and then Exclusion Rules . Click Remove link right the myExclusionRule . Note: you have to move mouse cursor into the myExclusionRule area to the see the Remove link. Confirm and click Yes, delete .","title":"Lab 3. Analyze Logs"},{"location":"logdna/lab-3/#analysing-logs-using-logdna","text":"","title":"Analysing logs using LogDNA"},{"location":"logdna/lab-3/#generate-application-log-entries","text":"The Petclinic application was de deployed in the previous exercice. Let's articially create some traffic in order to generate log statements. For simplicity, a script will be executed in api-gateway pod to send REST requests to all 4 service components of the petclinic application. Log entries will be generated for each service component.","title":"Generate application log entries"},{"location":"logdna/lab-3/#step-1-send-requests-to-the-sample-application","text":"Prior to sending the requests to the application, we will take a look at the pod logs and start watching the logs. This should provide an understanding of what comes in as log statements. To generate application log entries, Go back to CLI terminal and get a listing of pods. Then start watching the logs in separate CLI terminal sessions. kubectl get pods $ kubectl get pods NAME READY STATUS RESTARTS AGE api-gateway-575f59b7d8-bjhp8 1/1 Running 0 3h2m customers-687749cfb-d2zhq 1/1 Running 0 3h2m vets-6bb6655b7f-v6ppc 1/1 Running 0 3h2m visits-784749c647-tdpvv 1/1 Running 0 3h2m Watch logs for each of those pods using the follwing command: kubectl logs -f <pod-name> For instance the result of wathcing the api-gateway pod should like this: $ kubectl logs -f api-gateway-575f59b7d8-bjhp8 6.4.jar!/, jar:file:/app.jar!/BOOT-INF/lib/jquery-3.1.1-1.jar!/, jar:file:/app.jar!/BOOT-INF/lib/bootstrap-3.3.7-1.jar!/, jar:file:/app.jar!/BOOT-INF/lib/angular-ui-router-1.0.3 .jar!/, jar:file:/app.jar!/BOOT-INF/lib/webjars-locator-core-0.35.jar!/, jar:file:/app.jar!/BOOT-INF/lib/slf4j-api-1.7.26.jar!/, jar:file:/app.jar!/BOOT-INF/lib/commons-lang3-3. 8.1.jar!/, jar:file:/app.jar!/BOOT-INF/lib/commons-compress-1.9.jar!/, jar:file:/app.jar!/BOOT-INF/lib/jackson-core-2.9.8.jar!/] . ____ _ __ _ _ /\\\\ / ___'_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.1.4.RELEASE) 2021-02-11 19:58:03.730 INFO 1 --- [ main] o.s.s.p.api.ApiGatewayApplication : Starting ApiGatewayApplication v2.1.4 on api-gateway-575f59b7d8-bjhp8 with PID 1 (/app.jar started by root in /) 2021-02-11 19:58:03.731 DEBUG 1 --- [ main] o.s.s.p.api.ApiGatewayApplication : Running with Spring Boot v2.1.4.RELEASE, Spring v5.1.6.RELEASE 2021-02-11 19:58:03.731 INFO 1 --- [ main] o.s.s.p.api.ApiGatewayApplication : The following profiles are active: common,docker ..... ..... 2021-02-11 19:59:24.943 DEBUG 1 --- [nio-8080-exec-3] o.s.w.s.handler.SimpleUrlHandlerMapping : Mapped to ResourceHttpRequestHandler [\"classpath:/META-INF/resources/\", \"classpa th:/resources/\", \"classpath:/static/\", \"classpath:/public/\", \"/\"] 2021-02-11 19:59:24.943 DEBUG 1 --- [nio-8080-exec-1] o.s.w.s.handler.SimpleUrlHandlerMapping : Mapped to ResourceHttpRequestHandler [\"classpath:/META-INF/resources/\", \"classpa th:/resources/\", \"classpath:/static/\", \"classpath:/public/\", \"/\"] 2021-02-11 19:59:24.949 DEBUG 1 --- [nio-8080-exec-2] o.s.w.s.handler.SimpleUrlHandlerMapping : Mapped to ResourceHttpRequestHandler [\"classpath:/META-INF/resources/\", \"classpa th:/resources/\", \"classpath:/static/\", \"classpath:/public/\", \"/\"] 2021-02-11 19:59:24.967 DEBUG 1 --- [nio-8080-exec-3] o.s.web.servlet.DispatcherServlet : Completed 200 OK Go back to one of CLI terminal. Set the HOST to the Petclinic application URL. export HOST = \"https://petclinic. $INGRESS_SUBDOMAIN \" echo $HOST Run the follwing script to generate the log statements. This script will invoke the landing page, vets, owners and pet services in a loop. for i in ` seq 1 100 ` ; \\ do \\ echo -e \"\\n ======= Loop count: $i ========= \\nCalling owners:\" && \\ wget -q -O - \" ${ HOST } /api/customer/owners\" && \\ echo -e \"\\n Calling vets:\" && \\ wget -q -O - \" ${ HOST } /api/vet/vets\" && \\ echo -e \"\\n Calling visits:\" && \\ wget -q -O - \" ${ HOST } /api/visit/pets/visits?petId=7\" && \\ echo -e \"\\n Calling welcome:\" && \\ wget -q -O - \" ${ HOST } /#\\!/welcome\" ; \\ done ======= Loop count: 2 ========= Calling owners: [{\"id\":1,\"firstName\":\"George\",\"lastName\":\"Franklin\",\"address\":\"110 W. Liberty St.\",\"city\":\"Madison\",\"telephone\":\"6085551023\",\"pets\":[{\"id\":1,\"name\":\"Leo\",\"birthDate\":\"2010-09-07\",\"type\":{\"id\":1,\"name\":\"cat\"}}]},{\"id\":2,\"firstName\":\"Betty\",\"lastName\":\"Davis\",\"address\":\"638 Cardinal Ave.\",\"city\":\"Sun Prairie\",\"telephone\":\"6085551749\",\"pets\":[{\"id\":2,\"name\":\"Basil\",\"birthDate\":\"2012-08-06\",\"type\":{\"id\":6,\"name\":\"hamster\"}}]},{\"id\":3,\"firstName\":\"Eduardo\",\"lastName\":\"Rodriquez\",\"address\":\"2693 Commerce St.\",\"city\":\"McFarland\",\"telephone\":\"6085558763\",\"pets\":[{\"id\":4,\"name\":\"Jewel\",\"birthDate\":\"2010-03-07\",\"type\":{\"id\":2,\"name\":\"dog\"}},{\"id\":3,\"name\":\"Rosy\",\"birthDate\":\"2011-04-17\",\"type\":{\"id\":2,\"name\":\"dog\"}}]},{\"id\":4,\"firstName\":\"Harold\",\"lastName\":\"Davis\",\"address\":\"563 Friendly St.\",\"city\":\"Windsor\",\"telephone\":\"6085553198\",\"pets\":[{\"id\":5,\"name\":\"Iggy\",\"birthDate\":\"2010-11-30\",\"type\":{\"id\":3,\"name\":\"lizard\"}}]},{\"id\":5,\"firstName\":\"Peter\",\"lastName\":\"McTavish\",\"address\":\"2387 S. ......... ......... <body class=\"container\"> <layout-nav></layout-nav> <div class=\"container-fluid\"> <div class=\"container xd-container\"> <div ui-view=\"\"></div> </div> </div> <layout-footer></layout-footer> </body> </html> Switch to the CLI sessions setup for watching the los to view the logging activity.","title":"Step 1 - Send requests to the sample application"},{"location":"logdna/lab-3/#work-with-your-logs-via-logdna","text":"We are not ready to effectively analyze the application logs via LogDNA.","title":"Work with your logs via LogDNA"},{"location":"logdna/lab-3/#step-1-open-logdna-console","text":"Open the LogDNA console like we did in the prevous lab. LogDNA console displays everything by default. LogDNA live-streams all log entries by default. You may turn OFF/ON LIVE log stream via the LIVE toggle button at the bottom right corner.","title":"Step 1: Open LogDNA console"},{"location":"logdna/lab-3/#step-2-search-for-specific-terms-in-logs","text":"If you know what you are looking for, it's easy and convenient to search log entries in LogDNA. In the Search input box located at the bottom of the page in the LogDNA UI, enter GET \"/owners\" and press ENTER. Review the search result. All remaining log entries contains string GET \"/owners\" . Select any log entry and expand it by click the arraw in the front of the line. This displays detail information of the log entry. Select View in context . The log line will be displayed in context of other log lines from that host, app, or both. This information is helpful when troubleshooting a problem. Select EVERYTHING in the top-left corner to clear the searching criteria and restore all log entries. In the Search input box located at the bottom of the page in the LogDNA UI, enter level:error and press ENTER. Review the search result and only error log entries remain. Select EVERYTHING in the top-left corner to clear the searching criteria and restore all log entries. Enter 2 mins ago in the Jump to timeframe input box and press ENTER. LogDNA jumps to a specific timeframe, Click the icon next to the input box to find the other time formats within your retention period. Select the Toggle Viewer Tools icon at the bottom-right. Enter error as your highlight term in the first field and press ENTER. Enter container as your highlight term in the second field and press ENTER. Click the Toggle Viewer Tools icon to close the pop-up window. Click on Toggle Timeline icon to see log entries at a specific time of a day.","title":"Step 2: Search for specific terms in logs"},{"location":"logdna/lab-3/#step-3-filter-logs-for-a-specific-container","text":"You can filter logs by tags, sources, containers or levels. Tags: related to IKS clusters Sources: related to IKS pods Containers: related to IKS containers. Since one container typically runs one microservice per cloud native development best practice, each container should represent one microservice. For example, customers container represents customers service component. Levels: related to log level, for example error or debug. To filter logs, On LogDNA UI, select EVERYTHING in the top left corner to clear the searching criteria and restore all log entries. Select Tags dropdown list on the top. You can filter log entries related to a single cluster or a set of clusters. For this exercise, you don't have to change anything. Select Sources dropdown list, you may filter log entries related to one Kubernetes pod or a set of pods. For this exercise, you don't have to change anything. Select Apps dropdown list, you may filter log entries related to one Kubernetes container or a set of containers. Select Customers checkbox under the Containers section and Apply . Now, you see log entries for Customers service component only. Most of the entries are for DEBUG purpose. Select Levels dropdown list, you may filter log entries related to one log level or a set of log levels. Select everything except DEBUG log level and Apply . You may see no log entry or a few lines of log entries, depending on your log entries.","title":"Step 3 - Filter logs for a specific container"},{"location":"logdna/lab-3/#step-4-create-a-new-view","text":"You may create a new view to save your current filter. Click Unsaved View on the LogDNA UI and select Save as new view . In the Name field, enter My CUSTOMERS service component . Select Save View .","title":"Step 4 - Create a new view"},{"location":"logdna/lab-3/#step-5-visualize-logs-with-graphs-and-breakdowns","text":"In this section, you will create a board and then add a graph with a breakdown to visualize the app level data. A board is a collection of graphs and breakdowns. In the left pane, select the Boards icon and then select NEW BOARD . Click Edit icon ( Pencil icon next to the New Board title) on the top bar. In the Name field, enter Debug Board . Click Save . Click Add Graph . Under the Graph a field section, select level in the first field. Second field Field Value appears. Select debug in the Field Value field. Click Add Graph . Counts is selected as your metric by default. The graph shows the number of log entries in the interval over last 24 hours. Extend the graph by clicking on the downn-arrow below the graph. Select Histogram as your breakdown type . Choose app in the second field. Select Add Breakdown to see a breakdown with all the apps you logged.","title":"Step 5 - Visualize logs with graphs and breakdowns"},{"location":"logdna/lab-3/#step-6-alerts","text":"You can trigger alerts in LogDNA whenever log lines appear in a custom view. The sample alert configuration in this section sends an email for every 3 new log entries that meet searching criteria.","title":"Step 6 - Alerts"},{"location":"logdna/lab-3/#step-61-create-custom-view","text":"To create an alert in LogDNA, Navigate to the homepage of the LogDNA. Select EVERYTHING . Enter GET \"/owners\" in the Search field and press ENTER. Click Unsaved View button on the top-left and select Save as new view . Enter REST Call in the Name field. Select Save View button.","title":"Step 6.1 - Create custom view"},{"location":"logdna/lab-3/#step-62-create-alert","text":"Select Settings in the left pane. Select ALERTS tab. Select Add Preset button. Enter customer-api-called as the Preset name . Select Email option. Triggers the alert when there are 3 Lines appears in 30 seconds . Send an alert At the end of 30 seconds . Turn on Custom schedule and review the settings. Make change if necessary. Enter your email address. Select your Timezone . Click Test button next to the Email . You should receive an email from LogDNA Alerts with the following testing contents. Test View \ud83d\udd14 2 test lines matched Feb 08 17:45:56 logdna alert_tester This is where your lines will show up Feb 08 17:45:56 logdna alert_tester After matching at least 3 lines in a 30 second period, we'll send an alert to this email with all the matched lines Save Alert .","title":"Step 6.2 - Create alert"},{"location":"logdna/lab-3/#step-63-attach-alert-and-custom-view","text":"To link alert and custom view in LogDNA, Select Views tab in the left pane. Select REST Call custom view. Click the REST Call dropdown menu on the top-left. Select Attach an alert option. Select customer-api-called alert. Save Alert .","title":"Step 6.3 - Attach alert and custom view"},{"location":"logdna/lab-3/#step-64-trigger-alert","text":"To trigger the LogDNA alert, you'll generate additional application log entries. Go back to IBM Cloud Shell terminal. Copy/paste and Execute the following command in the pod. The script sends 3 requests to customer microservice. for i in `seq 1 3` ; \\ do \\ echo -e \"\\n ======= Loop count: $i ========= \\nCalling customer owners:\" && \\ wget -q -O - \"${HOST}/api/customer/owners\" && \\ done Verify you received an email from LogDNA Alerts . REST Call \ud83d\udd14 (end of duration) 3 lines matched within 30 seconds Feb 08 19:14:01 customers-98d7b966c-8wx5x customers [DEBUG] 1 --- [io-8080-exec-10] o.s.web.servlet.DispatcherServlet : GET \"/owners\", parameters={} Feb 08 19:14:01 customers-98d7b966c-8wx5x customers [DEBUG] 1 --- [nio-8080-exec-8] o.s.web.servlet.DispatcherServlet : GET \"/owners\", parameters={} Feb 08 19:14:01 customers-98d7b966c-8wx5x customers [DEBUG] 1 --- [nio-8080-exec-9] o.s.web.servlet.DispatcherServlet : GET \"/owners\", parameters={}","title":"Step 6.4 - Trigger alert"},{"location":"logdna/lab-3/#step-7-review-the-log-format","text":"When you launch the IBM Log Analysis with LogDNA web UI, log entries are displayed in a predefined format. You can modify how the log entries are displayed. Note: Configuration changes of the log format in this section will affect all defined views. It's possible to change log format for individual view. Select the Settings icon in the left pane. Select USER PREFERENCES . Select Log Format tab. Review the default log format and available log components. Do not make any change for this exercise. Change the log viewer text size by using the slider. To add items to log view, drag the available items from the bottom line to the top line. To rearrange the order of the items, drag and drop the items in the top line until you have your desired view.","title":"Step 7 - Review the log format"},{"location":"logdna/lab-3/#step-8-configure-exclusion-rules","text":"In an IBM Log Analysis with LogDNA instance, you can configure exclusion rules through the LogDNA web UI to stop logs from counting against your data usage quota and from being stored for search.","title":"Step 8 - Configure exclusion rules"},{"location":"logdna/lab-3/#step-81-create-exclusion-rules","text":"Select the Settings icon in the left pane. Select Usage and then Exclusion Rules . Click Add Rule button. The Create Rule section opens. Enter myExclusionRule in the field What is this rule for? . In the Apps field, select customers . In the Query field, enter level:debug . The query exludes the Debug log entries. Select Preserve these lines for live-tail and alerting to show through the live tail the log lines that are excluded. Notice that you can still use these log lines to set up an alert. Click Save .","title":"Step 8.1 - Create exclusion rules"},{"location":"logdna/lab-3/#step-82-test-exclusion-rules","text":"To test the exclusion rule, Go back to the LogDNA homepage. Select EVERYTHING . Click Apps dropdown and filter the logs by selecting customers checkbox under Containers . Apply . Go to IBM Cloud Shell terminal. Copy/paste and Execute the following command in the pod. for i in `seq 1 3` ; \\ do \\ echo -e \"\\n ======= Loop count: $i ========= \\nCalling customer owners:\" && \\ wget -q -O - \"${HOST}/api/customer/owners\" && \\ done Go back to LogDNA UI, you should receive no new log entry by looking at the timestamp of the last log entry. This confirms that the newly generated log entries do not count against your data usage. Verify you received an email from LogDNA Alerts . This confirms that you can still receive alert based on the newly generated log entries although they don't count against your data usage.","title":"Step 8.2 - Test exclusion rules"},{"location":"logdna/lab-3/#step-83-delete-exclusion-rules","text":"The exclusion rule myExclusionRule should be deleted as it was created fopr demostration reason. Otherwise, your LogDNA may not as you expect. Select the Settings icon in the left pane. Select Usage and then Exclusion Rules . Click Remove link right the myExclusionRule . Note: you have to move mouse cursor into the myExclusionRule area to the see the Remove link. Confirm and click Yes, delete .","title":"Step 8.3 - Delete exclusion rules"},{"location":"resources/ADMIN/","text":"Admin Guide \u00b6 This section is comprised of the following steps: Instructor Step 1. Instructor Step \u00b6 Things specific to instructors can go here.","title":"Admin Guide"},{"location":"resources/ADMIN/#admin-guide","text":"This section is comprised of the following steps: Instructor Step","title":"Admin Guide"},{"location":"resources/ADMIN/#1-instructor-step","text":"Things specific to instructors can go here.","title":"1. Instructor Step"},{"location":"resources/CONTRIBUTORS/","text":"Contributors \u00b6 Lee Zhang \u00b6 Rojan Jose \u00b6 Github: rojanjose Twitter: @rojanjose LinkedIn: rojanjose","title":"Contributors"},{"location":"resources/CONTRIBUTORS/#contributors","text":"","title":"Contributors"},{"location":"resources/CONTRIBUTORS/#lee-zhang","text":"","title":"Lee Zhang"},{"location":"resources/CONTRIBUTORS/#rojan-jose","text":"Github: rojanjose Twitter: @rojanjose LinkedIn: rojanjose","title":"Rojan Jose"},{"location":"resources/MKDOCS/","text":"mkdocs examples \u00b6 This page includes a few neat tricks that you can do with mkdocs . For a complete list of examples visit the mkdocs documentation . Code \u00b6 print ( \"hello world!\" ) Code with line numbers \u00b6 1 2 3 4 5 def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Code with highlights \u00b6 def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Code with tabs \u00b6 Tab Header #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } Another Tab Header #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } More tabs \u00b6 Windows If on windows download the Win32.zip file and install it. MacOS Run brew install foo . Linux Run apt-get install foo . Checklists \u00b6 Lorem ipsum dolor sit amet, consectetur adipiscing elit Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst Add a button \u00b6 Launch the lab Visit IBM Developer Sign up! Call outs \u00b6 Tip You can use note , abstract , info , tip , success , question warning , failure , danger , bug , quote or example . Note A note. Abstract An abstract. Info Some info. Success A success. Question A question. Warning A warning. Danger A danger. Example A example. Bug A bug. Call outs with code \u00b6 Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim. Formatting \u00b6 In addition to the usual italics , and bold there is now support for: highlighted underlined strike-through Tables \u00b6 OS or Application Username Password Windows VM Administrator foo Linux VM root bar Emojis \u00b6 Yes, these work. Images \u00b6 Nunc eu odio eleifend, blandit leo a, volutpat sapien right align image \u00b6 Nunc eu odio eleifend, blandit leo a, volutpat sapien","title":"mkdocs examples"},{"location":"resources/MKDOCS/#mkdocs-examples","text":"This page includes a few neat tricks that you can do with mkdocs . For a complete list of examples visit the mkdocs documentation .","title":"mkdocs examples"},{"location":"resources/MKDOCS/#code","text":"print ( \"hello world!\" )","title":"Code"},{"location":"resources/MKDOCS/#code-with-line-numbers","text":"1 2 3 4 5 def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Code with line numbers"},{"location":"resources/MKDOCS/#code-with-highlights","text":"def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Code with highlights"},{"location":"resources/MKDOCS/#code-with-tabs","text":"Tab Header #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } Another Tab Header #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; }","title":"Code with tabs"},{"location":"resources/MKDOCS/#more-tabs","text":"Windows If on windows download the Win32.zip file and install it. MacOS Run brew install foo . Linux Run apt-get install foo .","title":"More tabs"},{"location":"resources/MKDOCS/#checklists","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst","title":"Checklists"},{"location":"resources/MKDOCS/#add-a-button","text":"Launch the lab Visit IBM Developer Sign up!","title":"Add a button"},{"location":"resources/MKDOCS/#call-outs","text":"Tip You can use note , abstract , info , tip , success , question warning , failure , danger , bug , quote or example . Note A note. Abstract An abstract. Info Some info. Success A success. Question A question. Warning A warning. Danger A danger. Example A example. Bug A bug.","title":"Call outs"},{"location":"resources/MKDOCS/#call-outs-with-code","text":"Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim.","title":"Call outs with code"},{"location":"resources/MKDOCS/#formatting","text":"In addition to the usual italics , and bold there is now support for: highlighted underlined strike-through","title":"Formatting"},{"location":"resources/MKDOCS/#tables","text":"OS or Application Username Password Windows VM Administrator foo Linux VM root bar","title":"Tables"},{"location":"resources/MKDOCS/#emojis","text":"Yes, these work.","title":"Emojis"},{"location":"resources/MKDOCS/#images","text":"Nunc eu odio eleifend, blandit leo a, volutpat sapien","title":"Images"},{"location":"resources/MKDOCS/#right-align-image","text":"Nunc eu odio eleifend, blandit leo a, volutpat sapien","title":"right align image"},{"location":"resources/RESOURCES/","text":"Additional resources \u00b6 TBD \u00b6","title":"Additional Resources"},{"location":"resources/RESOURCES/#additional-resources","text":"","title":"Additional resources"},{"location":"resources/RESOURCES/#tbd","text":"","title":"TBD"},{"location":"setup/","text":"Setup \u00b6 Follow the steps listed in the section to complete the pre-requesites for the setting up the lab environment. Step 1: Sign up for IBM Cloud \u00b6 Ensure you have an IBM Cloud ID Step 2: Claim and Access IBM Cloud Kubernetes cluster \u00b6 Use the IBM Kubernetes cluster provided for the workshop. Alternatively, you can provision a cluster and use it for this lab. Find the private endpoint and VRF enablement requirements here . Login to IBM Cloud in a browser and select the appropirate account under which the cluster is provisioned. Navigate to https://cloud.ibm.com/kubernetes/clusters to see a list of available IKS clusters. Select your IKS cluster from the list to open it to land at the cluster Overview page. Observe the information under Summary and note how the endpoints are enabled. Step 3: Connecting to the Kubernetes cluster \u00b6 In the left pane of Kubernetes cluster details page, select the Access tab. This page provides CLI commands to setup your IKS cluster using a terminal environment. Click IBM Cloud Shell link next to your account number on the toolbar. It's on the top-right corner of the screen. This opens IBM Cloud Shell window in a new tab of your browser. Note that additions terminal sessions can be opened by clicking the + icon. Execute the CLI commands listed on the Access tab of your IKS cluster details ppage (see above) sequentially to connect to your cluster. $ ibmcloud login -a cloud.ibm.com -r us-south -g Default API endpoint: https://cloud.ibm.com Email> rj@gmail.com Password> Authenticating... OK Select an account: 1. Rojan J's Account (6ec1c2ec26xx49d6b5ad8b632e629c3d) 2. Amce Corp (2c426324dd934f71a2a053c9185791ff) <-> 6272168 Enter a number> 2 Targeted account Amce Corp (2c426324cc934f71a2a053c9185791ff) <-> 6272168 Targeted resource group Default Targeted region us-south API endpoint: https://cloud.ibm.com Region: us-south User: rj@gmail.com Account: Amce Corp (2c426324cc934f71a2a053c9185791ff) <-> 6272168 Resource group: Default CF API endpoint: Org: Space: $ ibmcloud ks cluster config --cluster c13mk2cd0pfrniovo4r0 .... OK The configuration for c13mk2cd0pfrniovo4r0 was downloaded successfully. Added context for c13mk2cd0pfrniovo4r0 to the current kubeconfig file. You can now execute 'kubectl' commands against your cluster. For example, run 'kubectl get nodes'. If you are accessing the cluster for the first time, 'kubectl' commands might fail for a few seconds while RBAC synchronizes. Use CLI command kubectl config current-context to verify the connection to your cluster before continue the exercise. Store the cluster id in an environment variable. Copy the id from the cluster Overview tab as shown in the picture below and set that as the value for the MYCLUSTER variable. export MYCLUSTER=<your cluster id> Verify the cluster id set correctly. echo $MYCLUSTER $ echo $MYCLUSTER c13mk2cd0pfrniovo4r0 This completes the necessary prereqs for the labs.","title":"Setup"},{"location":"setup/#setup","text":"Follow the steps listed in the section to complete the pre-requesites for the setting up the lab environment.","title":"Setup"},{"location":"setup/#step-1-sign-up-for-ibm-cloud","text":"Ensure you have an IBM Cloud ID","title":"Step 1: Sign up for IBM Cloud"},{"location":"setup/#step-2-claim-and-access-ibm-cloud-kubernetes-cluster","text":"Use the IBM Kubernetes cluster provided for the workshop. Alternatively, you can provision a cluster and use it for this lab. Find the private endpoint and VRF enablement requirements here . Login to IBM Cloud in a browser and select the appropirate account under which the cluster is provisioned. Navigate to https://cloud.ibm.com/kubernetes/clusters to see a list of available IKS clusters. Select your IKS cluster from the list to open it to land at the cluster Overview page. Observe the information under Summary and note how the endpoints are enabled.","title":"Step 2: Claim and Access IBM Cloud Kubernetes cluster"},{"location":"setup/#step-3-connecting-to-the-kubernetes-cluster","text":"In the left pane of Kubernetes cluster details page, select the Access tab. This page provides CLI commands to setup your IKS cluster using a terminal environment. Click IBM Cloud Shell link next to your account number on the toolbar. It's on the top-right corner of the screen. This opens IBM Cloud Shell window in a new tab of your browser. Note that additions terminal sessions can be opened by clicking the + icon. Execute the CLI commands listed on the Access tab of your IKS cluster details ppage (see above) sequentially to connect to your cluster. $ ibmcloud login -a cloud.ibm.com -r us-south -g Default API endpoint: https://cloud.ibm.com Email> rj@gmail.com Password> Authenticating... OK Select an account: 1. Rojan J's Account (6ec1c2ec26xx49d6b5ad8b632e629c3d) 2. Amce Corp (2c426324dd934f71a2a053c9185791ff) <-> 6272168 Enter a number> 2 Targeted account Amce Corp (2c426324cc934f71a2a053c9185791ff) <-> 6272168 Targeted resource group Default Targeted region us-south API endpoint: https://cloud.ibm.com Region: us-south User: rj@gmail.com Account: Amce Corp (2c426324cc934f71a2a053c9185791ff) <-> 6272168 Resource group: Default CF API endpoint: Org: Space: $ ibmcloud ks cluster config --cluster c13mk2cd0pfrniovo4r0 .... OK The configuration for c13mk2cd0pfrniovo4r0 was downloaded successfully. Added context for c13mk2cd0pfrniovo4r0 to the current kubeconfig file. You can now execute 'kubectl' commands against your cluster. For example, run 'kubectl get nodes'. If you are accessing the cluster for the first time, 'kubectl' commands might fail for a few seconds while RBAC synchronizes. Use CLI command kubectl config current-context to verify the connection to your cluster before continue the exercise. Store the cluster id in an environment variable. Copy the id from the cluster Overview tab as shown in the picture below and set that as the value for the MYCLUSTER variable. export MYCLUSTER=<your cluster id> Verify the cluster id set correctly. echo $MYCLUSTER $ echo $MYCLUSTER c13mk2cd0pfrniovo4r0 This completes the necessary prereqs for the labs.","title":"Step 3: Connecting to the Kubernetes cluster"},{"location":"sysdig/lab-0/","text":"Sysdig Introduction \u00b6 IBM Cloud Monitoring with Sysdig , a fully managed enterprise-grade monitoring service for application visibility, alerting, and troubleshooting. IBM Cloud Monitoring with Sysdig is used by enterprise development and IT teams that build, ship, and run business-critical applications at scale. Read the blog . Pre-requisites \u00b6 Claim your kubernetes cluster and complete the setup steps as described here . Exercise Steps \u00b6 Lab 1 - Sysdig Instance Setup Lab 2 - Explore features in Sysdig Monitor Lab 3 - Monitoring using customer metrics Related Links \u00b6 There is lots of great information, tutorials, articles, etc on the IBM Developer site as well as broader web. Here are a subset of good examples related to data understanding, visualization and processing: IBM Cloud Monitoring with Sysdig Documentation Sysdig Documentation Sysdig Blog Golden Signals","title":"Introduction"},{"location":"sysdig/lab-0/#sysdig-introduction","text":"IBM Cloud Monitoring with Sysdig , a fully managed enterprise-grade monitoring service for application visibility, alerting, and troubleshooting. IBM Cloud Monitoring with Sysdig is used by enterprise development and IT teams that build, ship, and run business-critical applications at scale. Read the blog .","title":"Sysdig Introduction"},{"location":"sysdig/lab-0/#pre-requisites","text":"Claim your kubernetes cluster and complete the setup steps as described here .","title":"Pre-requisites"},{"location":"sysdig/lab-0/#exercise-steps","text":"Lab 1 - Sysdig Instance Setup Lab 2 - Explore features in Sysdig Monitor Lab 3 - Monitoring using customer metrics","title":"Exercise Steps"},{"location":"sysdig/lab-0/#related-links","text":"There is lots of great information, tutorials, articles, etc on the IBM Developer site as well as broader web. Here are a subset of good examples related to data understanding, visualization and processing: IBM Cloud Monitoring with Sysdig Documentation Sysdig Documentation Sysdig Blog Golden Signals","title":"Related Links"},{"location":"sysdig/lab-1/","text":"Setup IBM Cloud Monitoring with Sysdig \u00b6 IBM Cloud Monitoring with Sysdig is a cloud-native, and container-intelligence management system that you can include as part of your IBM Cloud architecture. Use it to gain operational visibility into the performance and health of your applications, services, and platforms. It offers administrators, DevOps teams and developers full stack telemetry with advanced features to monitor and troubleshoot, define alerts, and design custom dashboards. Step 1: Deploy IBM Cloud Monitoring with Sysdig \u00b6 To deploy an instance of IBM Cloud Monitoring with Sysdig , Login to IBM Cloud in a browser. Click here open the Monitoring page and view the IBM Cloud Monitoring with Sysdig instances provisioned under your account. Select Create instance at the top-right corner. Select a location, for example Dallas (us-south) . (Pick the location where the IKS cluster resides.) Select Graduated Tier - Sysdig Secure + Monitor as the pricing plan. Enter a service name or accept the default. Leave Resource group as Default . Click the Create button to create the new service instance. Step 2: Connect your Kubernetes cluster to Sysdig \u00b6 Sysdig Agent collects metrics from Kubernetes pods and sends it the Sydig instance. The agnent should be installed on each pod of your Kubernetes to begin monitoring. The Sysdig agent is installed via a DaemonSet which ensures an instance of the agent is running on every worker node. Login to IBM Cloud in a browser. Open the IBM Cloud Shell and connect to the cluster as described in Step 3 here . Get the service key. Goto the Sysdig instance and click on Edit sources to get to this screen. Copy the command shown for Public endpoint and edit the line to include few changes. Change the value for sysdig_capture_enabled: true and add a new parameter --imageanalyzer as shwon in the command below. These changes relates to enabling Sysdig secure and Capture . Note that the Sysdig service key, endpoint and tags are sent in as paramters. Run the new command in the Cloud Shell terminal. curl -sL https://ibm.biz/install-sysdig-k8s-agent | bash -s -- -a SYSDIG_ACCESS_KEY b -c COLLECTOR_ENDPOINT -ac 'sysdig_capture_enabled: true' --imageanalyzer $ curl -sL https://ibm.biz/install-sysdig-k8s-agent | bash -s -- -a 4de2635d-8615-4721-a73d-9be869a16bdb -c ingest.us-south.monitoring.cloud.ibm.com -ac 'sysdig_capture_enabled: true' --imageanalyzer * Detecting operating system * Downloading Sysdig cluster role yaml * Downloading Sysdig config map yaml * Downloading Sysdig daemonset v2 yaml * Downloading Sysdig agent-slim daemonset v2 yaml * Downloading Sysdig Image Analyzer config map yaml * Downloading Sysdig Image Analyzer daemonset v1 yaml * Creating namespace: ibm-observe * Creating sysdig-agent serviceaccount in namespace: ibm-observe * Creating sysdig-agent clusterrole and binding clusterrole.rbac.authorization.k8s.io/sysdig-agent created * Creating sysdig-agent secret using the ACCESS_KEY provided * Retreiving the IKS Cluster ID and Cluster Name * Setting cluster name as iks-cluster-user-0/c18i9ihd018m2dgi1vo0 * Setting ibm.containers-kubernetes.cluster.id c18i9ihd018m2dgi1vo0 * Updating agent configmap and applying to cluster * Setting tags * Setting collector endpoint * Adding additional configuration to dragent.yaml * Enabling Prometheus * Configuring Analysis Manager endpoint to https://ingest.us-south.monitoring.cloud.ibm.com/internal/scanning/scanning-analysis-collector. You can also use the -am option to explicitly specify it. * Setting Analysis Manager endpoint for Image Analyzer configmap/sysdig-image-analyzer created Processing all-icr-io as all-icr-io secret/all-icr-io created configmap/sysdig-agent created * Deploying the sysdig agent daemonset.apps/sysdig-agent created ... The agent install step creates a set of resources under ibm-observe namespace. To view those changes, run the command as shown below from the CLI environment: kubectl get all -n ibm-observe $ kubectl get all -n ibm-observe NAME READY STATUS RESTARTS AGE pod/sysdig-agent-kdp52 0/1 ContainerCreating 0 2m36s pod/sysdig-image-analyzer-zjhz7 0/1 ContainerCreating 0 2m35s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/sysdig-agent 1 1 0 1 0 <none> 2m36s daemonset.apps/sysdig-image-analyzer 1 1 0 1 0 <none> 2m35s Wait until the two agent pods for agent and image-analyzer are in the Running state. One agent is installed one per worker node. Additional pods will be listed for the LogDNA service, if installed. Step 3: Accessing Sysdig console \u00b6 Go back to the monitoring page and click on the View Sysdig link to open the landing page. The Explore tab is open by default. We are now ready to explore various features of Sysdig.","title":"Lab 1. Sysdig Install"},{"location":"sysdig/lab-1/#setup-ibm-cloud-monitoring-with-sysdig","text":"IBM Cloud Monitoring with Sysdig is a cloud-native, and container-intelligence management system that you can include as part of your IBM Cloud architecture. Use it to gain operational visibility into the performance and health of your applications, services, and platforms. It offers administrators, DevOps teams and developers full stack telemetry with advanced features to monitor and troubleshoot, define alerts, and design custom dashboards.","title":"Setup IBM Cloud Monitoring with Sysdig"},{"location":"sysdig/lab-1/#step-1-deploy-ibm-cloud-monitoring-with-sysdig","text":"To deploy an instance of IBM Cloud Monitoring with Sysdig , Login to IBM Cloud in a browser. Click here open the Monitoring page and view the IBM Cloud Monitoring with Sysdig instances provisioned under your account. Select Create instance at the top-right corner. Select a location, for example Dallas (us-south) . (Pick the location where the IKS cluster resides.) Select Graduated Tier - Sysdig Secure + Monitor as the pricing plan. Enter a service name or accept the default. Leave Resource group as Default . Click the Create button to create the new service instance.","title":"Step 1: Deploy IBM Cloud Monitoring with Sysdig"},{"location":"sysdig/lab-1/#step-2-connect-your-kubernetes-cluster-to-sysdig","text":"Sysdig Agent collects metrics from Kubernetes pods and sends it the Sydig instance. The agnent should be installed on each pod of your Kubernetes to begin monitoring. The Sysdig agent is installed via a DaemonSet which ensures an instance of the agent is running on every worker node. Login to IBM Cloud in a browser. Open the IBM Cloud Shell and connect to the cluster as described in Step 3 here . Get the service key. Goto the Sysdig instance and click on Edit sources to get to this screen. Copy the command shown for Public endpoint and edit the line to include few changes. Change the value for sysdig_capture_enabled: true and add a new parameter --imageanalyzer as shwon in the command below. These changes relates to enabling Sysdig secure and Capture . Note that the Sysdig service key, endpoint and tags are sent in as paramters. Run the new command in the Cloud Shell terminal. curl -sL https://ibm.biz/install-sysdig-k8s-agent | bash -s -- -a SYSDIG_ACCESS_KEY b -c COLLECTOR_ENDPOINT -ac 'sysdig_capture_enabled: true' --imageanalyzer $ curl -sL https://ibm.biz/install-sysdig-k8s-agent | bash -s -- -a 4de2635d-8615-4721-a73d-9be869a16bdb -c ingest.us-south.monitoring.cloud.ibm.com -ac 'sysdig_capture_enabled: true' --imageanalyzer * Detecting operating system * Downloading Sysdig cluster role yaml * Downloading Sysdig config map yaml * Downloading Sysdig daemonset v2 yaml * Downloading Sysdig agent-slim daemonset v2 yaml * Downloading Sysdig Image Analyzer config map yaml * Downloading Sysdig Image Analyzer daemonset v1 yaml * Creating namespace: ibm-observe * Creating sysdig-agent serviceaccount in namespace: ibm-observe * Creating sysdig-agent clusterrole and binding clusterrole.rbac.authorization.k8s.io/sysdig-agent created * Creating sysdig-agent secret using the ACCESS_KEY provided * Retreiving the IKS Cluster ID and Cluster Name * Setting cluster name as iks-cluster-user-0/c18i9ihd018m2dgi1vo0 * Setting ibm.containers-kubernetes.cluster.id c18i9ihd018m2dgi1vo0 * Updating agent configmap and applying to cluster * Setting tags * Setting collector endpoint * Adding additional configuration to dragent.yaml * Enabling Prometheus * Configuring Analysis Manager endpoint to https://ingest.us-south.monitoring.cloud.ibm.com/internal/scanning/scanning-analysis-collector. You can also use the -am option to explicitly specify it. * Setting Analysis Manager endpoint for Image Analyzer configmap/sysdig-image-analyzer created Processing all-icr-io as all-icr-io secret/all-icr-io created configmap/sysdig-agent created * Deploying the sysdig agent daemonset.apps/sysdig-agent created ... The agent install step creates a set of resources under ibm-observe namespace. To view those changes, run the command as shown below from the CLI environment: kubectl get all -n ibm-observe $ kubectl get all -n ibm-observe NAME READY STATUS RESTARTS AGE pod/sysdig-agent-kdp52 0/1 ContainerCreating 0 2m36s pod/sysdig-image-analyzer-zjhz7 0/1 ContainerCreating 0 2m35s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/sysdig-agent 1 1 0 1 0 <none> 2m36s daemonset.apps/sysdig-image-analyzer 1 1 0 1 0 <none> 2m35s Wait until the two agent pods for agent and image-analyzer are in the Running state. One agent is installed one per worker node. Additional pods will be listed for the LogDNA service, if installed.","title":"Step 2: Connect your Kubernetes cluster to Sysdig"},{"location":"sysdig/lab-1/#step-3-accessing-sysdig-console","text":"Go back to the monitoring page and click on the View Sysdig link to open the landing page. The Explore tab is open by default. We are now ready to explore various features of Sysdig.","title":"Step 3: Accessing Sysdig console"},{"location":"sysdig/lab-2/","text":"Monitoring Kubernetes infrastructure with Sysdig Monitor \u00b6 As a pre-requisite to this lab, complete the setup instructions desribed here . Deploy sample Petclinic application to IKS cluster as described here in this exercise . Skip the optional MySQL section. This exercise deploys the application and sets the ingress routes to access the application and the related microservices. Step 1: Overview of the infrastructure \u00b6 The Overview tab provided a unified view of health, risk, capacity of the infrastructure. Select the Overview tab and pick Clusters from the list of options. This view shows the overall health of the clusters being monitored. Hover over the space shown in the red arrow to get additional context menu. Use the options in this menu for further analysis. Click on the Overview tab and select Workloads to segment the view based on the workloads. Choose the Namespace ibm-observe to view the workloads belonging to the Sysdig. Step 2: Using Dashboards \u00b6 Dashboards provides you a collection of relevant views and metrics for the infrastructure in a single view. Let's try couple of views from the existing list of templates. Select the Dashboard tab and pick Containers > Containers Resource Usage . Select the Dashboard tab again and pick Troubleshooting > Process Resource Usage . To add your custome view click on the Dashboards tab and select the (+) icon. Step 3: Troubleshooting with Explore \u00b6 This tab helps you view and troubleshoot key metrics and entities of your infrastructure stack. Explore net.http.request.count To test this out, let's atrificially trigger some traffic to the Customer service. Set the HOST variable and invoke the REST service API in a loop: export HOST=\"https://petclinic.$INGRESS_SUBDOMAIN\" echo $HOST for i in `seq 1 1000` ; \\ do \\ echo -e \"\\n ======= Loop count: $i ========= \\nCalling owners:\" && \\ wget -q -O - \"${HOST}/api/customer/owners\" ; done Click on the Explore tab and select Containerized Apps . In the Metrics dropdown list, select net.http.request.count to view the request spikes. View Response times between containers Select the Response Times dashboard to get a general view of network traffic between the contianers and the response times. Step 4: Setting up Alerts \u00b6 Alerts generate notifications based on certain conditions or events that requires further attention. Crash the Vists service. $ kubectl get pods NAME READY STATUS RESTARTS AGE api-gateway-575f59b7d8-vlm6x 1/1 Running 0 115m customers-687749cfb-vzblv 1/1 Running 0 115m vets-6bb6655b7f-dpf88 1/1 Running 0 115m visits-784749c647-rxh2n 1/1 Running 0 115m $ kubectl delete pods visits-784749c647-rxh2n pod \"visits-784749c647-rxh2n\" deleted Setup notification channel Open the settings menu from the top left bottom. Add a Notification Channel of type Email . Adding a new Alert Click on the Alerts tab and then click on (+) Add Alert at the top right. Select Event as the Alert Type . Enter ExitCode = 143 for Tag or Description , Source Tag value as containerd . Enable the notification channel Visits Service Crash and click on the CREATE button. Rerun the Visits service crash set to trigger an alert. You will receive separate notifications for Triggered and Resolved status. Step 5: Analysing Events \u00b6 The Events view displays a comprehensive list of events that have occurred in the environment. Let's create one additional event prior to looking at the Events view. noimage-service.yaml poiints to a non-existent image. Create the Image Pull error event cd $HOME/kubernetes-logging-and-monitoring/src $ kubectl create -f k8s/monitor/noimage-service.yaml deployment.apps/missing-image created service/missing-image-service created The pod with missing image will show a status of ErrImagePull . kubectl get pods NAME READY STATUS RESTARTS AGE api-gateway-575f59b7d8-vlm6x 1/1 Running 0 11h customers-687749cfb-vzblv 1/1 Running 0 11h missing-image-6c677574d8-zqc57 0/1 ErrImagePull 0 54s vets-6bb6655b7f-dpf88 1/1 Running 0 11h visits-784749c647-t6tb9 1/1 Running 0 9h View the events Open the Events view to find Back Off Container Start or Image Pull and Container Image Pull, Create or Start Failed at the top of the events list. Select the Event to get additional details. Step 6: Creating Captures \u00b6 The Captures tab gives a list of capture files with system calls and OS events for further analysis. Create a Capture Go to the Explore view. From the list of Containerized Apps , select the customer-service docker image. Then select Sysdig Capture from the top right menu. Enter a name for the capture and click START CAPTURE .","title":"Lab 2. Explore Monitor"},{"location":"sysdig/lab-2/#monitoring-kubernetes-infrastructure-with-sysdig-monitor","text":"As a pre-requisite to this lab, complete the setup instructions desribed here . Deploy sample Petclinic application to IKS cluster as described here in this exercise . Skip the optional MySQL section. This exercise deploys the application and sets the ingress routes to access the application and the related microservices.","title":"Monitoring Kubernetes infrastructure with Sysdig Monitor"},{"location":"sysdig/lab-2/#step-1-overview-of-the-infrastructure","text":"The Overview tab provided a unified view of health, risk, capacity of the infrastructure. Select the Overview tab and pick Clusters from the list of options. This view shows the overall health of the clusters being monitored. Hover over the space shown in the red arrow to get additional context menu. Use the options in this menu for further analysis. Click on the Overview tab and select Workloads to segment the view based on the workloads. Choose the Namespace ibm-observe to view the workloads belonging to the Sysdig.","title":"Step 1: Overview of the infrastructure"},{"location":"sysdig/lab-2/#step-2-using-dashboards","text":"Dashboards provides you a collection of relevant views and metrics for the infrastructure in a single view. Let's try couple of views from the existing list of templates. Select the Dashboard tab and pick Containers > Containers Resource Usage . Select the Dashboard tab again and pick Troubleshooting > Process Resource Usage . To add your custome view click on the Dashboards tab and select the (+) icon.","title":"Step 2: Using Dashboards"},{"location":"sysdig/lab-2/#step-3-troubleshooting-with-explore","text":"This tab helps you view and troubleshoot key metrics and entities of your infrastructure stack. Explore net.http.request.count To test this out, let's atrificially trigger some traffic to the Customer service. Set the HOST variable and invoke the REST service API in a loop: export HOST=\"https://petclinic.$INGRESS_SUBDOMAIN\" echo $HOST for i in `seq 1 1000` ; \\ do \\ echo -e \"\\n ======= Loop count: $i ========= \\nCalling owners:\" && \\ wget -q -O - \"${HOST}/api/customer/owners\" ; done Click on the Explore tab and select Containerized Apps . In the Metrics dropdown list, select net.http.request.count to view the request spikes. View Response times between containers Select the Response Times dashboard to get a general view of network traffic between the contianers and the response times.","title":"Step 3: Troubleshooting with Explore"},{"location":"sysdig/lab-2/#step-4-setting-up-alerts","text":"Alerts generate notifications based on certain conditions or events that requires further attention. Crash the Vists service. $ kubectl get pods NAME READY STATUS RESTARTS AGE api-gateway-575f59b7d8-vlm6x 1/1 Running 0 115m customers-687749cfb-vzblv 1/1 Running 0 115m vets-6bb6655b7f-dpf88 1/1 Running 0 115m visits-784749c647-rxh2n 1/1 Running 0 115m $ kubectl delete pods visits-784749c647-rxh2n pod \"visits-784749c647-rxh2n\" deleted Setup notification channel Open the settings menu from the top left bottom. Add a Notification Channel of type Email . Adding a new Alert Click on the Alerts tab and then click on (+) Add Alert at the top right. Select Event as the Alert Type . Enter ExitCode = 143 for Tag or Description , Source Tag value as containerd . Enable the notification channel Visits Service Crash and click on the CREATE button. Rerun the Visits service crash set to trigger an alert. You will receive separate notifications for Triggered and Resolved status.","title":"Step 4: Setting up Alerts"},{"location":"sysdig/lab-2/#step-5-analysing-events","text":"The Events view displays a comprehensive list of events that have occurred in the environment. Let's create one additional event prior to looking at the Events view. noimage-service.yaml poiints to a non-existent image. Create the Image Pull error event cd $HOME/kubernetes-logging-and-monitoring/src $ kubectl create -f k8s/monitor/noimage-service.yaml deployment.apps/missing-image created service/missing-image-service created The pod with missing image will show a status of ErrImagePull . kubectl get pods NAME READY STATUS RESTARTS AGE api-gateway-575f59b7d8-vlm6x 1/1 Running 0 11h customers-687749cfb-vzblv 1/1 Running 0 11h missing-image-6c677574d8-zqc57 0/1 ErrImagePull 0 54s vets-6bb6655b7f-dpf88 1/1 Running 0 11h visits-784749c647-t6tb9 1/1 Running 0 9h View the events Open the Events view to find Back Off Container Start or Image Pull and Container Image Pull, Create or Start Failed at the top of the events list. Select the Event to get additional details.","title":"Step 5: Analysing Events"},{"location":"sysdig/lab-2/#step-6-creating-captures","text":"The Captures tab gives a list of capture files with system calls and OS events for further analysis. Create a Capture Go to the Explore view. From the list of Containerized Apps , select the customer-service docker image. Then select Sysdig Capture from the top right menu. Enter a name for the capture and click START CAPTURE .","title":"Step 6: Creating Captures"},{"location":"sysdig/lab-3/","text":"Monitoring with custom metrics \u00b6 In this exercice, we will see how to generatge custom metircs using a Prometheus client and mock multiple access to API endpoints. The sample application called WoLaM extracted from the tutorial here . This project includes the code to generate custom metrics. For instance, review line 29 here to see how the custom metric wolam_api_counter is defined. Step 1: Setup the application \u00b6 Setup manifests for the application Ensure the valeu of INGRESS_SUBDOMAIN is set based on the sample application deploy steps from this lab . echo \"Ingress subdomain: ${INGRESS_SUBDOMAIN}\" Replace <INGRESS_SUBDOMAIN> in k8s/ingress.yaml file with the value that you retrieved in the previous step by running the following command: sed -i \"s/<INGRESS_SUBDOMAIN>/ ${ INGRESS_SUBDOMAIN } /\" k8s/monitor/app-log-analysis.yaml Users on Mac: sed -i \"\" \"s/<INGRESS_SUBDOMAIN>/ ${ INGRESS_SUBDOMAIN } /\" k8s/monitor/app-log-analysis.yaml Verify the igress value was substituted correctly: $ more k8s/app-log-analysis.yaml ... spec: rules: - host: observe.iks-cluster-user-0-c074815f6c3dc41618f978f8bffffa07-0000.us-south.containers.appdomain.cloud http: paths: - path: /(.*) ... Run the applicaiton Run the kubectl command to deploy the application. Two pods with name app-log-analysis-deployment should show in Running status. kubectl create -f k8s/monitor/app-log-analysis.yaml deployment.apps/app-log-analysis-deployment created service/app-log-analysis-svc created ingress.networking.k8s.io/app-log-analysis-ingress created $ kubectl get pods NAME READY STATUS RESTARTS AGE api-gateway-575f59b7d8-vlm6x 1/1 Running 0 12h app-log-analysis-deployment-595df69c5-77l5h 1/1 Running 0 20s app-log-analysis-deployment-595df69c5-gxwvh 1/1 Running 0 20s customers-687749cfb-vzblv 1/1 Running 0 12h missing-image-6c677574d8-zqc57 0/1 ImagePullBackOff 0 78m vets-6bb6655b7f-dpf88 1/1 Running 0 12h visits-784749c647-t6tb9 1/1 Running 0 10h Use the application endpoint to get to the WoLaM landing page. echo \"https://observe.${INGRESS_SUBDOMAIN}\" echo \"https://observe.${INGRESS_SUBDOMAIN}\" https://observe.iks-cluster-user-0-c074815f6c3dc41618f978f8bffffa07-0000.us-south.containers.appdomain.cloud The application is now ready to send custom metric data. Step 2: Monitor Metrics \u00b6 Generate metrics Go to the WoLaM application, select the Monitoring tab and generate few metrics. for eg: Development , London & 25 . View metrics To monitor the calls to a given api endpoint of the application, From the Explore tab, select Deployments. Select Metrics > Prometheus > wolam_api_counter_total in the Metrics and Dashboards dropdown. Select Time: Average, Group: Sum, Segment: endpoint . Go back to the WoLaM application, generate a few metrics after changing the region. To monitor the calls to a given api endpoint of the application by region, Select Time: Average, Group: Sum, Segment: region .","title":"Lab 3. Custom Metrics"},{"location":"sysdig/lab-3/#monitoring-with-custom-metrics","text":"In this exercice, we will see how to generatge custom metircs using a Prometheus client and mock multiple access to API endpoints. The sample application called WoLaM extracted from the tutorial here . This project includes the code to generate custom metrics. For instance, review line 29 here to see how the custom metric wolam_api_counter is defined.","title":"Monitoring with custom metrics"},{"location":"sysdig/lab-3/#step-1-setup-the-application","text":"Setup manifests for the application Ensure the valeu of INGRESS_SUBDOMAIN is set based on the sample application deploy steps from this lab . echo \"Ingress subdomain: ${INGRESS_SUBDOMAIN}\" Replace <INGRESS_SUBDOMAIN> in k8s/ingress.yaml file with the value that you retrieved in the previous step by running the following command: sed -i \"s/<INGRESS_SUBDOMAIN>/ ${ INGRESS_SUBDOMAIN } /\" k8s/monitor/app-log-analysis.yaml Users on Mac: sed -i \"\" \"s/<INGRESS_SUBDOMAIN>/ ${ INGRESS_SUBDOMAIN } /\" k8s/monitor/app-log-analysis.yaml Verify the igress value was substituted correctly: $ more k8s/app-log-analysis.yaml ... spec: rules: - host: observe.iks-cluster-user-0-c074815f6c3dc41618f978f8bffffa07-0000.us-south.containers.appdomain.cloud http: paths: - path: /(.*) ... Run the applicaiton Run the kubectl command to deploy the application. Two pods with name app-log-analysis-deployment should show in Running status. kubectl create -f k8s/monitor/app-log-analysis.yaml deployment.apps/app-log-analysis-deployment created service/app-log-analysis-svc created ingress.networking.k8s.io/app-log-analysis-ingress created $ kubectl get pods NAME READY STATUS RESTARTS AGE api-gateway-575f59b7d8-vlm6x 1/1 Running 0 12h app-log-analysis-deployment-595df69c5-77l5h 1/1 Running 0 20s app-log-analysis-deployment-595df69c5-gxwvh 1/1 Running 0 20s customers-687749cfb-vzblv 1/1 Running 0 12h missing-image-6c677574d8-zqc57 0/1 ImagePullBackOff 0 78m vets-6bb6655b7f-dpf88 1/1 Running 0 12h visits-784749c647-t6tb9 1/1 Running 0 10h Use the application endpoint to get to the WoLaM landing page. echo \"https://observe.${INGRESS_SUBDOMAIN}\" echo \"https://observe.${INGRESS_SUBDOMAIN}\" https://observe.iks-cluster-user-0-c074815f6c3dc41618f978f8bffffa07-0000.us-south.containers.appdomain.cloud The application is now ready to send custom metric data.","title":"Step 1: Setup the application"},{"location":"sysdig/lab-3/#step-2-monitor-metrics","text":"Generate metrics Go to the WoLaM application, select the Monitoring tab and generate few metrics. for eg: Development , London & 25 . View metrics To monitor the calls to a given api endpoint of the application, From the Explore tab, select Deployments. Select Metrics > Prometheus > wolam_api_counter_total in the Metrics and Dashboards dropdown. Select Time: Average, Group: Sum, Segment: endpoint . Go back to the WoLaM application, generate a few metrics after changing the region. To monitor the calls to a given api endpoint of the application by region, Select Time: Average, Group: Sum, Segment: region .","title":"Step 2: Monitor Metrics"}]}